# LLM-G-Code-Generator

**Official implementation of the paper: "Large language models for G-Code generation in design for additive manufacturing"**

This repository provides an end-to-end framework for generating 3D printing G-code directly from design parameters using Large Language Models (LLMs). All scripts are located in the root directory for ease of use.

ðŸš€ Usage Guide

1. Installation
Install the required dependencies:
pip install -r requirements.txt

2. Model Fine-tuning
Run the training script for the desired model (e.g., Qwen1-G-Coder):
python model_training_Qwen1-G-Coder.py

3. Model Evaluation
Calculate the Cross Entropy Loss and Perplexity (PPL) on the validation and test sets (e.g., Qwen1-G-Coder):
python model_evaluate_Qwen1-G-Coder.py

4. Model Inference
Generate raw G-code using the fine-tuned model (e.g., Qwen1-G-Coder):
python model_inference_Qwen1-G-Coder.py

5. Post-Processing
To generate a printable .gcode file, you must run the following three scripts in order:

Step 1: Add Header and Footer 
Completes the G-code with machine start/end sequences.
python G-code-post-processing-complete.py

Step 2: Path Filling 
Optimizes the single-layer path for closed loops.
python G-code-post-processing-single_layer_filled.py

Step 3: Multi-layer Stacking 
Extrudes the 2D path into a 3D volume (e.g., 20 layers) to create the final printable file.
python G-code-post-processing-multi_layers_printable.py


ðŸ’¾ Dataset
The dataset is located in the dataset/ directory. It contains 5,898 pairs of parametric gear designs (z, m, d) and their corresponding Ground Truth G-codes.


## Directory Structure

```text
.
â”œâ”€â”€ dataset/                                # Dataset folder
â”‚   â””â”€â”€ gear_gcode_dataset.jsonl            # Parameter-to-G-code dataset
â”‚
â”œâ”€â”€ requirements.txt                        # Python dependencies
â”‚
â”œâ”€â”€ [Model Fine-tuning Scripts]
â”œâ”€â”€ model_training_Qwen1.py                 # Fine-tuning script for Qwen1-G-Coder
â”œâ”€â”€ model_training_Qwen2.py                 # Fine-tuning script for Qwen2-G-Coder
â”œâ”€â”€ model_training_DeepSeek-G-Coder.py      # Fine-tuning script for DeepSeek-G-Coder
â”‚
â”œâ”€â”€ [Model Inference Scripts]
â”œâ”€â”€ model_inference_Qwen1.py                # Inference script for Qwen1-G-Coder
â”œâ”€â”€ model_inference_Qwen2.py                # Inference script for Qwen2-G-Coder
â”œâ”€â”€ model_inference_DeepSeek-G-Coder.py     # Inference script for DeepSeek-G-Coder
â”‚
â”œâ”€â”€ [Model Evaluation Scripts]
â”œâ”€â”€ model_evaluate_Qwen1.py                 # Evaluation script for Qwen1-G-Coder
â”œâ”€â”€ model_evaluate_Qwen2.py                 # Evaluation script for Qwen2-G-Coder
â”œâ”€â”€ model_evaluate_DeepSeek-G-Coder.py      # Evaluation script for DeepSeek-G-Coder
â”‚
â”œâ”€â”€ [Post-Processing Scripts] (Run in order)
â”œâ”€â”€ G-code-post-processing-complete.py               # Step 1: Add Header/Footer
â”œâ”€â”€ G-code-post-processing-single_layer_filled.py    # Step 2: Path Filling
â””â”€â”€ G-code-post-processing-multi_layers_printable.py # Step 3: Multi-layer Stacking (Final Output)
â”‚
â”œâ”€â”€ [Results]
â”œâ”€â”€ results/                                # Generated outputs
    â”œâ”€â”€ raw_outputs/                        # Raw LLM outputs (Core path only)
    â””â”€â”€ printable_gcodes/                   # Final printable G-codes



